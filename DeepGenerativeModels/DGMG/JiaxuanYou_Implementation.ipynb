{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import shuffle\n",
    "import networkx as nx\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np \n",
    "import os \n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import time as tm \n",
    "import random \n",
    "import pickle as pkl \n",
    "import scipy.sparse as sp\n",
    "\n",
    "#import dgl\n",
    "import torch.nn as nn\n",
    "# tensorboard --logdir=tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:33:07.146442Z",
     "start_time": "2024-06-21T08:33:03.982399500Z"
    }
   },
   "id": "d16145a9ebacf58b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation from DeepGenerativeModels.GraphRNN_master\n",
    "[From Github Repo](https://github.com/JiaxuanYou/graph-generation/blob/master/main_DeepGMG.py)\n",
    "\n",
    "Note: \n",
    "**networkx==1.11** was used \n",
    "File ~\\PycharmProjects\\MA\\venv\\lib\\site-packages\\networkx\\algorithms\\dag.py:2\n",
    "from math import gcd # not fractions\n",
    "The code-modifications only work with 1.11 as well, which makes them redundant in the first place.  \n",
    "\n",
    "Methodology: \n",
    "**The batches are graphs, but no node features are considered**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9f496686fb38860"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data (Load and save)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0e8cbed1361009"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def caveman_special(c=2,k=20,p_path=0.1,p_edge=0.3):\n",
    "    p = p_path\n",
    "    path_count = max(int(np.ceil(p * k)),1)\n",
    "    G = nx.caveman_graph(c, k)\n",
    "    # remove 50% edges\n",
    "    p = 1-p_edge\n",
    "    for (u, v) in list(G.edges()):\n",
    "        if np.random.rand() < p and ((u < k and v < k) or (u >= k and v >= k)):\n",
    "            G.remove_edge(u, v)\n",
    "    # add path_count links\n",
    "    for i in range(path_count):\n",
    "        u = np.random.randint(0, k)\n",
    "        v = np.random.randint(k, k * 2)\n",
    "        G.add_edge(u, v)\n",
    "    \n",
    "    G = max([G.subgraph(c).copy() for c in nx.connected_components(G)], key=len)\n",
    "    return G\n",
    "\n",
    "def save_graph_list(G_list, fname):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pkl.dump(G_list, f)\n",
    "        \n",
    "# load ENZYMES and PROTEIN and DD dataset\n",
    "def Graph_load_batch(min_num_nodes = 20, max_num_nodes = 1000, name = 'ENZYMES',node_attributes = True,graph_labels=True):\n",
    "    '''\n",
    "    load many graphs, e.g. enzymes\n",
    "    :return: a list of graphs\n",
    "    '''\n",
    "    print('Loading graph dataset: '+str(name))\n",
    "    G = nx.Graph()\n",
    "    # load data\n",
    "    path = 'dataset/'+name+'/'\n",
    "    data_adj = np.loadtxt(path+name+'_A.txt', delimiter=',').astype(int)\n",
    "    if node_attributes:\n",
    "        data_node_att = np.loadtxt(path+name+'_node_attributes.txt', delimiter=',')\n",
    "    data_node_label = np.loadtxt(path+name+'_node_labels.txt', delimiter=',').astype(int)\n",
    "    data_graph_indicator = np.loadtxt(path+name+'_graph_indicator.txt', delimiter=',').astype(int)\n",
    "    if graph_labels:\n",
    "        data_graph_labels = np.loadtxt(path+name+'_graph_labels.txt', delimiter=',').astype(int)\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def Graph_load(dataset = 'cora'):\n",
    "    '''\n",
    "    Load a single graph dataset\n",
    "    :param dataset: dataset name\n",
    "    :return:\n",
    "    '''\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        load = pkl.load(open(\"dataset/ind.{}.{}\".format(dataset, names[i]), 'rb'), encoding='latin1')\n",
    "        # print('loaded')\n",
    "        objects.append(load)\n",
    "        # print(load)\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"dataset/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    G = nx.from_dict_of_lists(graph)\n",
    "    adj = nx.adjacency_matrix(G)\n",
    "    return adj, features, G\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:33:18.692987600Z",
     "start_time": "2024-06-21T08:33:18.653904700Z"
    }
   },
   "id": "ed878248c59369c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22afe924a59c04fc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def calc_graph_embedding(node_embedding_cat, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param node_embedding_cat: \n",
    "    :param model: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    node_embedding_graph = model.f_m(node_embedding_cat)\n",
    "    node_embedding_graph_gate = model.f_gate(node_embedding_cat)\n",
    "    graph_embedding = torch.sum(torch.mul(node_embedding_graph, node_embedding_graph_gate), dim=0, keepdim=True)\n",
    "    return graph_embedding\n",
    "\n",
    "def calc_init_embedding(node_embedding_cat, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param node_embedding_cat: \n",
    "    :param model: An instantiated nn.module \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    node_embedding_init = model.f_m_init(node_embedding_cat)\n",
    "    node_embedding_init_gate = model.f_gate_init(node_embedding_cat)\n",
    "    init_embedding = torch.sum(torch.mul(node_embedding_init, node_embedding_init_gate), dim=0, keepdim=True)\n",
    "    init_embedding = model.f_init(init_embedding)\n",
    "    return init_embedding\n",
    "\n",
    "def message_passing(node_neighbor, node_embedding, model):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param node_neighbor: degree (freind, of a friend, of a friend -> 3) \n",
    "    :param node_embedding: \n",
    "    :param model: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    node_embedding_new = []\n",
    "    for i in range(len(node_neighbor)):\n",
    "        neighbor_num = len(node_neighbor[i])\n",
    "        if neighbor_num > 0:\n",
    "            node_self = node_embedding[i].expand(neighbor_num, node_embedding[i].size(1))\n",
    "            node_self_neighbor = torch.cat([node_embedding[j] for j in node_neighbor[i]], dim=0)\n",
    "            message = torch.sum(model.m_uv_1(torch.cat((node_self, node_self_neighbor), dim=1)), dim=0, keepdim=True)\n",
    "            node_embedding_new.append(model.f_n_1(message, node_embedding[i]))\n",
    "        else:\n",
    "            message_null = Variable(torch.zeros((node_embedding[i].size(0),node_embedding[i].size(1)*2)))\n",
    "            node_embedding_new.append(model.f_n_1(message_null, node_embedding[i]))\n",
    "    node_embedding = node_embedding_new\n",
    "    node_embedding_new = []\n",
    "    for i in range(len(node_neighbor)):\n",
    "        neighbor_num = len(node_neighbor[i])\n",
    "        if neighbor_num > 0:\n",
    "            node_self = node_embedding[i].expand(neighbor_num, node_embedding[i].size(1))\n",
    "            node_self_neighbor = torch.cat([node_embedding[j] for j in node_neighbor[i]], dim=0)\n",
    "            message = torch.sum(model.m_uv_1(torch.cat((node_self, node_self_neighbor), dim=1)), dim=0, keepdim=True)\n",
    "            node_embedding_new.append(model.f_n_1(message, node_embedding[i]))\n",
    "        else:\n",
    "            message_null = Variable(torch.zeros((node_embedding[i].size(0), node_embedding[i].size(1) * 2)))\n",
    "            node_embedding_new.append(model.f_n_1(message_null, node_embedding[i]))\n",
    "    return node_embedding_new\n",
    "\n",
    "def sample_tensor(y,sample=True, thresh=0.5):\n",
    "    # do sampling\n",
    "    if sample:\n",
    "        y_thresh = Variable(torch.rand(y.size()))\n",
    "        y_result = torch.gt(y,y_thresh).float()\n",
    "    # do max likelihood based on some threshold\n",
    "    else:\n",
    "        y_thresh = Variable(torch.ones(y.size())*thresh)\n",
    "        y_result = torch.gt(y, y_thresh).float()\n",
    "    return y_result\n",
    "\n",
    "def gumbel_softmax(logits, temperature, eps=1e-9):\n",
    "    '''\n",
    "\n",
    "    :param logits: shape: N*L\n",
    "    :param temperature:\n",
    "    :param eps:\n",
    "    :return:\n",
    "    '''\n",
    "    # get gumbel noise\n",
    "    noise = torch.rand(logits.size())\n",
    "    noise.add_(eps).log_().neg_()\n",
    "    noise.add_(eps).log_().neg_()\n",
    "    noise = Variable(noise)\n",
    "\n",
    "    x = (logits + noise) / temperature\n",
    "    x = F.softmax(x)\n",
    "    return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:33:21.853962100Z",
     "start_time": "2024-06-21T08:33:21.822721200Z"
    }
   },
   "id": "9214cf87091469f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model with arguments "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24a1ea3aabfc6d7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DGM_graphs(nn.Module):\n",
    "    def __init__(self,h_size):\n",
    "        # h_size: node embedding size\n",
    "        # h_size*2: graph embedding size\n",
    "\n",
    "        super(DGM_graphs, self).__init__()\n",
    "        ### all modules used by the model\n",
    "        ## 1 message passing, 2 times\n",
    "        self.m_uv_1 = nn.Linear(h_size*2, h_size*2)\n",
    "        self.f_n_1 = nn.GRUCell(h_size*2, h_size) # input_size, hidden_size\n",
    "\n",
    "        self.m_uv_2 = nn.Linear(h_size * 2, h_size * 2)\n",
    "        self.f_n_2 = nn.GRUCell(h_size * 2, h_size)  # input_size, hidden_size\n",
    "\n",
    "        ## 2 graph embedding and new node embedding\n",
    "        # for graph embedding\n",
    "        self.f_m = nn.Linear(h_size, h_size*2)\n",
    "        self.f_gate = nn.Sequential(\n",
    "            nn.Linear(h_size,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # for new node embedding\n",
    "        self.f_m_init = nn.Linear(h_size, h_size*2)\n",
    "        self.f_gate_init = nn.Sequential(\n",
    "            nn.Linear(h_size,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.f_init = nn.Linear(h_size*2, h_size)\n",
    "\n",
    "        ## 3 f_addnode\n",
    "        self.f_an = nn.Sequential(\n",
    "            nn.Linear(h_size*2,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        ## 4 f_addedge\n",
    "        self.f_ae = nn.Sequential(\n",
    "            nn.Linear(h_size * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        ## 5 f_nodes\n",
    "        self.f_s = nn.Linear(h_size*2, 1)\n",
    "\n",
    "class Args_DGMG():\n",
    "    def __init__(self):\n",
    "        ### CUDA\n",
    "        self.cuda = 0\n",
    "\n",
    "        ### model type\n",
    "        self.note = 'Baseline_DGMG' # do GCN after adding each edge\n",
    "        # self.note = 'Baseline_DGMG_fast' # do GCN only after adding each node\n",
    "\n",
    "        ### data config\n",
    "        self.graph_type = 'caveman_small'\n",
    "        # self.graph_type = 'grid_small'\n",
    "        # self.graph_type = 'ladder_small'\n",
    "        # self.graph_type = 'enzymes_small'\n",
    "        # self.graph_type = 'barabasi_small'\n",
    "        # self.graph_type = 'citeseer_small'\n",
    "\n",
    "        self.max_num_node = 20\n",
    "\n",
    "        ### network config\n",
    "        self.node_embedding_size = 64\n",
    "        self.test_graph_num = 200\n",
    "\n",
    "\n",
    "        ### training config\n",
    "        self.epochs = 2000  # now one epoch means self.batch_ratio x batch_size\n",
    "        self.load_epoch = 2000\n",
    "        self.epochs_test_start = 100\n",
    "        self.epochs_test = 100\n",
    "        self.epochs_log = 100\n",
    "        self.epochs_save = 100\n",
    "        if 'fast' in self.note:\n",
    "            self.is_fast = True\n",
    "        else:\n",
    "            self.is_fast = False\n",
    "\n",
    "        self.lr = 0.001\n",
    "        self.milestones = [300, 600, 1000]\n",
    "        self.lr_rate = 0.3\n",
    "\n",
    "        ### output config\n",
    "        self.model_save_path = 'model_save/'\n",
    "        self.graph_save_path = 'graphs/'\n",
    "        self.figure_save_path = 'figures/'\n",
    "        self.timing_save_path = 'timing/'\n",
    "        self.figure_prediction_save_path = 'figures_prediction/'\n",
    "        self.nll_save_path = 'nll/'\n",
    "\n",
    "\n",
    "        self.fname = self.note + '_' + self.graph_type + '_' + str(self.node_embedding_size)\n",
    "        self.fname_pred = self.note + '_' + self.graph_type + '_' + str(self.node_embedding_size) + '_pred_'\n",
    "        self.fname_train = self.note + '_' + self.graph_type + '_' + str(self.node_embedding_size) + '_train_'\n",
    "        self.fname_test = self.note + '_' + self.graph_type + '_' + str(self.node_embedding_size) + '_test_'\n",
    "\n",
    "        self.load = False\n",
    "        self.save = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:33:26.539113700Z",
     "start_time": "2024-06-21T08:33:26.507871Z"
    }
   },
   "id": "983e5dd1649a5170"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and test loops "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "942af122cfe054ea"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_DGMG_epoch(epoch, args, model, dataset, optimizer, scheduler, is_fast = False):\n",
    "    model.train()\n",
    "    graph_num = len(dataset)\n",
    "    order = list(range(graph_num))\n",
    "    shuffle(order)\n",
    "\n",
    "\n",
    "    loss_addnode = 0\n",
    "    loss_addedge = 0\n",
    "    loss_node = 0\n",
    "    for i in order:\n",
    "        model.zero_grad()\n",
    "\n",
    "        graph = dataset[i]\n",
    "        # do random ordering: relabel nodes\n",
    "        node_order = list(range(graph.number_of_nodes()))\n",
    "        shuffle(node_order)\n",
    "        order_mapping = dict(zip(graph.nodes(), node_order))\n",
    "        graph = nx.relabel_nodes(graph, order_mapping, copy=True)\n",
    "\n",
    "\n",
    "        # NOTE: when starting loop, we assume a node has already been generated\n",
    "        node_count = 1\n",
    "        node_embedding = [Variable(torch.ones(1,args.node_embedding_size))] \n",
    "        # list of torch tensors, each size: 1*hidden\n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        while node_count<=graph.number_of_nodes():\n",
    "            # .adjacency_list()  deprecated now: .adj or list(G.adjacency())\n",
    "            # adjacency_list = [[n for n in G.neighbors(node)] for node in G.nodes()]\n",
    "            # The induced subgraph of the graph contains the nodes in nodes and the edges between those nodes.\n",
    "            \n",
    "        \n",
    "            node_neighbor = [[n for n in graph.subgraph(list(range(node_count))).neighbors(node)]\n",
    "                             for node in graph.subgraph(list(range(node_count))).nodes()]\n",
    "            # list of lists (first node is zero)\n",
    " \n",
    "            node_neighbor_new = [[n for n in graph.subgraph(list(range(node_count+1))).neighbors(node)]\n",
    "                                 for node in graph.subgraph(list(range(node_count+1))).nodes()][-1]\n",
    "            #node_neighbor_new = graph.subgraph(list(range(node_count+1))).adj[-1] # list of new node's neighbors\n",
    "            print(node_neighbor_new)\n",
    "            \n",
    "            # 1 message passing\n",
    "            # do 2 times message passing\n",
    "            node_embedding = message_passing(node_neighbor, node_embedding, model)\n",
    "\n",
    "            # 2 graph embedding and new node embedding\n",
    "            node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "            graph_embedding = calc_graph_embedding(node_embedding_cat, model)\n",
    "            init_embedding = calc_init_embedding(node_embedding_cat, model)\n",
    "\n",
    "            # 3 f_addnode\n",
    "            p_addnode = model.f_an(graph_embedding)\n",
    "            if node_count < graph.number_of_nodes():\n",
    "                # add node\n",
    "                node_neighbor.append([])\n",
    "                node_embedding.append(init_embedding)\n",
    "                if is_fast:\n",
    "                    node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "                # calc loss\n",
    "                loss_addnode_step = F.binary_cross_entropy(p_addnode,Variable(torch.ones((1,1))))\n",
    "                # loss_addnode_step.backward(retain_graph=True)\n",
    "                loss += loss_addnode_step\n",
    "                loss_addnode += loss_addnode_step.data\n",
    "            else:\n",
    "                # calc loss\n",
    "                loss_addnode_step = F.binary_cross_entropy(p_addnode, Variable(torch.zeros((1, 1))))\n",
    "                # loss_addnode_step.backward(retain_graph=True)\n",
    "                loss += loss_addnode_step\n",
    "                loss_addnode += loss_addnode_step.data\n",
    "                break\n",
    "\n",
    "\n",
    "            edge_count = 0\n",
    "            while edge_count<=len(node_neighbor_new):\n",
    "                if not is_fast:\n",
    "                    node_embedding = message_passing(node_neighbor, node_embedding, model)\n",
    "                    node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "                    graph_embedding = calc_graph_embedding(node_embedding_cat, model)\n",
    "\n",
    "                # 4 f_addedge\n",
    "                p_addedge = model.f_ae(graph_embedding)\n",
    "\n",
    "                if edge_count < len(node_neighbor_new):\n",
    "                    # calc loss\n",
    "                    loss_addedge_step = F.binary_cross_entropy(p_addedge, Variable(torch.ones((1, 1))))\n",
    "                    # loss_addedge_step.backward(retain_graph=True)\n",
    "                    loss += loss_addedge_step\n",
    "                    loss_addedge += loss_addedge_step.data\n",
    "\n",
    "                    # 5 f_nodes\n",
    "                    # excluding the last node (which is the new node)\n",
    "                    node_new_embedding_cat = node_embedding_cat[-1,:].expand(node_embedding_cat.size(0)-1,node_embedding_cat.size(1))\n",
    "                    s_node = model.f_s(torch.cat((node_embedding_cat[0:-1,:],node_new_embedding_cat),dim=1))\n",
    "                    p_node = F.softmax(s_node.permute(1,0), dim=1)\n",
    "                    # get ground truth\n",
    "                    a_node = torch.zeros((1,p_node.size(1)))\n",
    "                    # print('node_neighbor_new',node_neighbor_new, edge_count)\n",
    "                    a_node[0,node_neighbor_new[edge_count]] = 1\n",
    "                    a_node = Variable(a_node)\n",
    "                    # add edge\n",
    "                    node_neighbor[-1].append(node_neighbor_new[edge_count])\n",
    "                    node_neighbor[node_neighbor_new[edge_count]].append(len(node_neighbor)-1)\n",
    "                    # calc loss\n",
    "                    loss_node_step = F.binary_cross_entropy(p_node,a_node)\n",
    "                    # loss_node_step.backward(retain_graph=True)\n",
    "                    loss += loss_node_step\n",
    "                    loss_node += loss_node_step.data\n",
    "\n",
    "                else:\n",
    "                    # calc loss\n",
    "                    loss_addedge_step = F.binary_cross_entropy(p_addedge, Variable(torch.zeros((1, 1))))\n",
    "                    # loss_addedge_step.backward(retain_graph=True)\n",
    "                    loss += loss_addedge_step\n",
    "                    loss_addedge += loss_addedge_step.data\n",
    "                    break\n",
    "\n",
    "                edge_count += 1\n",
    "            node_count += 1\n",
    "\n",
    "        # update deterministic and lstm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_all = loss_addnode + loss_addedge + loss_node\n",
    "\n",
    "    if epoch % args.epochs_log==0:\n",
    "        print('Epoch: {}/{}, train loss: {:.6f}, graph type: {}, hidden: {}'.format(\n",
    "            epoch, args.epochs,loss_all[0], args.graph_type, args.node_embedding_size))\n",
    "\n",
    "\n",
    "    # loss_sum += loss.data[0]*x.size(0)\n",
    "    # return loss_sum\n",
    "\n",
    "\n",
    "def train_DGMG_forward_epoch(args, model, dataset, is_fast = False):\n",
    "    model.train()\n",
    "    graph_num = len(dataset)\n",
    "    order = list(range(graph_num))\n",
    "    shuffle(order)\n",
    "\n",
    "\n",
    "    loss_addnode = 0\n",
    "    loss_addedge = 0\n",
    "    loss_node = 0\n",
    "    for i in order:\n",
    "        model.zero_grad()\n",
    "\n",
    "        graph = dataset[i]\n",
    "        # do random ordering: relabel nodes\n",
    "        node_order = list(range(graph.number_of_nodes()))\n",
    "        shuffle(node_order)\n",
    "        order_mapping = dict(zip(graph.nodes(), node_order))\n",
    "        graph = nx.relabel_nodes(graph, order_mapping, copy=True)\n",
    "\n",
    "\n",
    "        # NOTE: when starting loop, we assume a node has already been generated\n",
    "        node_count = 1\n",
    "        node_embedding = [Variable(torch.ones(1,args.node_embedding_size))] # list of torch tensors, each size: 1*hidden\n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        while node_count<=graph.number_of_nodes():\n",
    "            \n",
    "            node_neighbor = [[n for n in graph.subgraph(list(range(node_count))).neighbors(node)]\n",
    "                             for node in graph.subgraph(list(range(node_count))).nodes()]\n",
    "            # list of lists (first node is zero)\n",
    " \n",
    "            node_neighbor_new = [[n for n in graph.subgraph(list(range(node_count+1))).neighbors(node)]\n",
    "                                 for node in graph.subgraph(list(range(node_count+1))).nodes()][-1]\n",
    "            # list of new node's neighbors\n",
    "            print(node_neighbor_new)\n",
    "\n",
    "            # 1 message passing\n",
    "            # do 2 times message passing\n",
    "            node_embedding = message_passing(node_neighbor, node_embedding, model)\n",
    "\n",
    "            # 2 graph embedding and new node embedding\n",
    "            node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "            graph_embedding = calc_graph_embedding(node_embedding_cat, model)\n",
    "            init_embedding = calc_init_embedding(node_embedding_cat, model)\n",
    "\n",
    "            # 3 f_addnode\n",
    "            p_addnode = model.f_an(graph_embedding)\n",
    "            if node_count < graph.number_of_nodes():\n",
    "                # add node\n",
    "                node_neighbor.append([])\n",
    "                node_embedding.append(init_embedding)\n",
    "                if is_fast:\n",
    "                    node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "                # calc loss\n",
    "                loss_addnode_step = F.binary_cross_entropy(p_addnode,Variable(torch.ones((1,1))))\n",
    "                # loss_addnode_step.backward(retain_graph=True)\n",
    "                loss += loss_addnode_step\n",
    "                loss_addnode += loss_addnode_step.data\n",
    "            else:\n",
    "                # calc loss\n",
    "                loss_addnode_step = F.binary_cross_entropy(p_addnode, Variable(torch.zeros((1, 1))))\n",
    "                # loss_addnode_step.backward(retain_graph=True)\n",
    "                loss += loss_addnode_step\n",
    "                loss_addnode += loss_addnode_step.data\n",
    "                break\n",
    "\n",
    "\n",
    "            edge_count = 0\n",
    "            while edge_count<=len(node_neighbor_new):\n",
    "                if not is_fast:\n",
    "                    node_embedding = message_passing(node_neighbor, node_embedding, model)\n",
    "                    node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "                    graph_embedding = calc_graph_embedding(node_embedding_cat, model)\n",
    "\n",
    "                # 4 f_addedge\n",
    "                p_addedge = model.f_ae(graph_embedding)\n",
    "\n",
    "                if edge_count < len(node_neighbor_new):\n",
    "                    # calc loss\n",
    "                    loss_addedge_step = F.binary_cross_entropy(p_addedge, Variable(torch.ones((1, 1))))\n",
    "                    # loss_addedge_step.backward(retain_graph=True)\n",
    "                    loss += loss_addedge_step\n",
    "                    loss_addedge += loss_addedge_step.data\n",
    "\n",
    "                    # 5 f_nodes\n",
    "                    # excluding the last node (which is the new node)\n",
    "                    node_new_embedding_cat = node_embedding_cat[-1,:].expand(node_embedding_cat.size(0)-1,node_embedding_cat.size(1))\n",
    "                    s_node = model.f_s(torch.cat((node_embedding_cat[0:-1,:],node_new_embedding_cat),dim=1))\n",
    "                    p_node = F.softmax(s_node.permute(1,0), dim=1)\n",
    "                    # get ground truth\n",
    "                    a_node = torch.zeros((1,p_node.size(1)))\n",
    "                    # print('node_neighbor_new',node_neighbor_new, edge_count)\n",
    "                    a_node[0,node_neighbor_new[edge_count]] = 1\n",
    "                    a_node = Variable(a_node)\n",
    "                    # add edge\n",
    "                    node_neighbor[-1].append(node_neighbor_new[edge_count])\n",
    "                    node_neighbor[node_neighbor_new[edge_count]].append(len(node_neighbor)-1)\n",
    "                    # calc loss\n",
    "                    loss_node_step = F.binary_cross_entropy(p_node,a_node)\n",
    "                    # loss_node_step.backward(retain_graph=True)\n",
    "                    loss += loss_node_step\n",
    "                    loss_node += loss_node_step.data*p_node.size(1)\n",
    "\n",
    "                else:\n",
    "                    # calc loss\n",
    "                    loss_addedge_step = F.binary_cross_entropy(p_addedge, Variable(torch.zeros((1, 1))))\n",
    "                    # loss_addedge_step.backward(retain_graph=True)\n",
    "                    loss += loss_addedge_step\n",
    "                    loss_addedge += loss_addedge_step.data\n",
    "                    break\n",
    "\n",
    "                edge_count += 1\n",
    "            node_count += 1\n",
    "\n",
    "\n",
    "    loss_all = loss_addnode + loss_addedge + loss_node\n",
    "\n",
    "    # if epoch % args.epochs_log==0:\n",
    "    #     print('Epoch: {}/{}, train loss: {:.6f}, graph type: {}, hidden: {}'.format(\n",
    "    #         epoch, args.epochs,loss_all[0], args.graph_type, args.node_embedding_size))\n",
    "\n",
    "\n",
    "    return loss_all[0]/len(dataset)\n",
    "\n",
    "\n",
    "def test_DGMG_epoch(args, model, is_fast=False):\n",
    "    model.eval()\n",
    "    graph_num = args.test_graph_num\n",
    "\n",
    "    graphs_generated = []\n",
    "    for i in range(graph_num):\n",
    "        # NOTE: when starting loop, we assume a node has already been generated\n",
    "        node_neighbor = [[]]  # list of lists (first node is zero)\n",
    "        node_embedding = [Variable(torch.ones(1,args.node_embedding_size))] \n",
    "        # list of torch tensors, each size: 1*hidden\n",
    "\n",
    "        node_count = 1\n",
    "        while node_count<=args.max_num_node:\n",
    "            # 1 message passing\n",
    "            # do 2 times message passing\n",
    "            node_embedding = message_passing(node_neighbor, node_embedding, model)\n",
    "\n",
    "            # 2 graph embedding and new node embedding\n",
    "            node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "            graph_embedding = calc_graph_embedding(node_embedding_cat, model)\n",
    "            init_embedding = calc_init_embedding(node_embedding_cat, model)\n",
    "\n",
    "            # 3 f_addnode\n",
    "            p_addnode = model.f_an(graph_embedding)\n",
    "            a_addnode = sample_tensor(p_addnode)\n",
    "            # print(a_addnode.data[0][0])\n",
    "            if a_addnode.data[0][0]==1:\n",
    "                # print('add node')\n",
    "                # add node\n",
    "                node_neighbor.append([])\n",
    "                node_embedding.append(init_embedding)\n",
    "                if is_fast:\n",
    "                    node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            edge_count = 0\n",
    "            while edge_count<args.max_num_node:\n",
    "                if not is_fast:\n",
    "                    node_embedding = message_passing(node_neighbor, node_embedding, model)\n",
    "                    node_embedding_cat = torch.cat(node_embedding, dim=0)\n",
    "                    graph_embedding = calc_graph_embedding(node_embedding_cat, model)\n",
    "\n",
    "                # 4 f_addedge\n",
    "                p_addedge = model.f_ae(graph_embedding)\n",
    "                a_addedge = sample_tensor(p_addedge)\n",
    "                # print(a_addedge.data[0][0])\n",
    "\n",
    "                if a_addedge.data[0][0]==1:\n",
    "                    # print('add edge')\n",
    "                    # 5 f_nodes\n",
    "                    # excluding the last node (which is the new node)\n",
    "                    node_new_embedding_cat = node_embedding_cat[-1,:].expand(node_embedding_cat.size(0)-1,node_embedding_cat.size(1))\n",
    "                    s_node = model.f_s(torch.cat((node_embedding_cat[0:-1,:],node_new_embedding_cat),dim=1))\n",
    "                    p_node = F.softmax(s_node.permute(1,0), dim=1)\n",
    "                    a_node = gumbel_softmax(p_node, temperature=0.01)\n",
    "                    _, a_node_id = a_node.topk(1)\n",
    "                    a_node_id = int(a_node_id.data[0][0])\n",
    "                    # add edge\n",
    "                    node_neighbor[-1].append(a_node_id)\n",
    "                    node_neighbor[a_node_id].append(len(node_neighbor)-1)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                edge_count += 1\n",
    "            node_count += 1\n",
    "        # save graph\n",
    "        node_neighbor_dict = dict(zip(list(range(len(node_neighbor))), node_neighbor))\n",
    "        graph = nx.from_dict_of_lists(node_neighbor_dict)\n",
    "        graphs_generated.append(graph)\n",
    "\n",
    "    return graphs_generated\n",
    "\n",
    "\n",
    "\n",
    "########### train function for LSTM + VAE\n",
    "def train_DGMG(args, dataset_train, model):\n",
    "    # check if load existing model\n",
    "    if args.load:\n",
    "        fname = args.model_save_path + args.fname + 'model_' + str(args.load_epoch) + '.dat'\n",
    "        model.load_state_dict(torch.load(fname))\n",
    "\n",
    "        args.lr = 0.00001\n",
    "        epoch = args.load_epoch\n",
    "        print('model loaded!, lr: {}'.format(args.lr))\n",
    "    else:\n",
    "        epoch = 1\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=args.lr)\n",
    "\n",
    "    scheduler = MultiStepLR(optimizer, milestones=args.milestones, gamma=args.lr_rate)\n",
    "\n",
    "    # start main loop\n",
    "    time_all = np.zeros(args.epochs)\n",
    "    while epoch <= args.epochs:\n",
    "        time_start = tm.time()\n",
    "        # train\n",
    "        train_DGMG_epoch(epoch, args, model, dataset_train, optimizer, scheduler, is_fast=args.is_fast)\n",
    "        time_end = tm.time()\n",
    "        time_all[epoch - 1] = time_end - time_start\n",
    "        # print('time used',time_all[epoch - 1])\n",
    "        # test\n",
    "        if epoch % args.epochs_test == 0 and epoch >= args.epochs_test_start:\n",
    "            graphs = test_DGMG_epoch(args,model, is_fast=args.is_fast)\n",
    "            fname = args.graph_save_path + args.fname_pred + str(epoch) + '.dat'\n",
    "            save_graph_list(graphs, fname)\n",
    "            # print('test done, graphs saved')\n",
    "\n",
    "        # save model checkpoint\n",
    "        if args.save:\n",
    "            if epoch % args.epochs_save == 0:\n",
    "                fname = args.model_save_path + args.fname + 'model_' + str(epoch) + '.dat'\n",
    "                torch.save(model.state_dict(), fname)\n",
    "        epoch += 1\n",
    "    np.save(args.timing_save_path + args.fname, time_all)\n",
    "\n",
    "\n",
    "########### train function for LSTM + VAE\n",
    "def train_DGMG_nll(args, dataset_train,dataset_test, model,max_iter=1000):\n",
    "    # check if load existing model\n",
    "    fname = args.model_save_path + args.fname + 'model_' + str(args.load_epoch) + '.dat'\n",
    "    model.load_state_dict(torch.load(fname))\n",
    "\n",
    "    fname_output = args.nll_save_path + args.note + '_' + args.graph_type + '.csv'\n",
    "    with open(fname_output, 'w+') as f:\n",
    "        f.write('train,test\\n')\n",
    "        # start main loop\n",
    "        for iter in range(max_iter):\n",
    "            nll_train = train_DGMG_forward_epoch(args, model, dataset_train, is_fast=args.is_fast)\n",
    "            nll_test = train_DGMG_forward_epoch(args, model, dataset_test, is_fast=args.is_fast)\n",
    "            print('train', nll_train, 'test', nll_test)\n",
    "            f.write(str(nll_train) + ',' + str(nll_test) + '\\n')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:33:39.220503100Z",
     "start_time": "2024-06-21T08:33:39.159991900Z"
    }
   },
   "id": "9363d07249b47486"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 0\n",
      "File name prefix Baseline_DGMG_caveman_small_64\n",
      "!\n",
      "total graph num: 100, training set: 80\n",
      "max number node: 20\n",
      "max previous node: 20\n",
      "[]\n",
      "[0]\n",
      "[]\n",
      "[2]\n",
      "[3]\n",
      "[0, 2, 4, 1]\n",
      "[0, 6, 4, 1]\n",
      "[5, 8]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for dimension 1 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 91\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax previous node: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(args\u001B[38;5;241m.\u001B[39mmax_prev_node))\n\u001B[0;32m     90\u001B[0m \u001B[38;5;66;03m### train\u001B[39;00m\n\u001B[1;32m---> 91\u001B[0m \u001B[43mtrain_DGMG\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgraphs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 363\u001B[0m, in \u001B[0;36mtrain_DGMG\u001B[1;34m(args, dataset_train, model)\u001B[0m\n\u001B[0;32m    361\u001B[0m time_start \u001B[38;5;241m=\u001B[39m tm\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    362\u001B[0m \u001B[38;5;66;03m# train\u001B[39;00m\n\u001B[1;32m--> 363\u001B[0m \u001B[43mtrain_DGMG_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_fast\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    364\u001B[0m time_end \u001B[38;5;241m=\u001B[39m tm\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    365\u001B[0m time_all[epoch \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m time_end \u001B[38;5;241m-\u001B[39m time_start\n",
      "Cell \u001B[1;32mIn[5], line 100\u001B[0m, in \u001B[0;36mtrain_DGMG_epoch\u001B[1;34m(epoch, args, model, dataset, optimizer, scheduler, is_fast)\u001B[0m\n\u001B[0;32m     98\u001B[0m a_node \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;241m1\u001B[39m,p_node\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m)))\n\u001B[0;32m     99\u001B[0m \u001B[38;5;66;03m# print('node_neighbor_new',node_neighbor_new, edge_count)\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m \u001B[43ma_node\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mnode_neighbor_new\u001B[49m\u001B[43m[\u001B[49m\u001B[43medge_count\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    101\u001B[0m a_node \u001B[38;5;241m=\u001B[39m Variable(a_node)\n\u001B[0;32m    102\u001B[0m \u001B[38;5;66;03m# add edge\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 8 is out of bounds for dimension 1 with size 8"
     ]
    }
   ],
   "source": [
    "args = Args_DGMG()\n",
    "# self.graph_type = 'caveman_small' \n",
    "# self.node_embedding_size = 64\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda)\n",
    "print('CUDA', args.cuda)\n",
    "print('File name prefix',args.fname)\n",
    "\n",
    "\n",
    "graphs = []\n",
    "for i in range(4, 10):\n",
    "    graphs.append(nx.ladder_graph(i))\n",
    "    \n",
    "model = DGM_graphs(h_size = args.node_embedding_size)\n",
    "\n",
    "if args.graph_type == 'ladder_small':\n",
    "    graphs = []\n",
    "    for i in range(2, 11):\n",
    "        graphs.append(nx.ladder_graph(i))\n",
    "    args.max_prev_node = 10\n",
    " \n",
    "if args.graph_type=='caveman_small':\n",
    "    print(\"!\")\n",
    "    graphs = []\n",
    "    for i in range(2, 3):\n",
    "        for j in range(6, 11):\n",
    "            for k in range(20):\n",
    "                graphs.append(caveman_special(i, j, p_edge=0.8))\n",
    "    args.max_prev_node = 20\n",
    "    \n",
    "if args.graph_type == 'grid_small':\n",
    "    graphs = []\n",
    "    for i in range(2, 5):\n",
    "        for j in range(2, 6):\n",
    "            graphs.append(nx.grid_2d_graph(i, j))\n",
    "    args.max_prev_node = 15\n",
    "    \n",
    "if args.graph_type == 'barabasi_small':\n",
    "    graphs = []\n",
    "    for i in range(4, 21):\n",
    "        for j in range(3, 4):\n",
    "            for k in range(10):\n",
    "                graphs.append(nx.barabasi_albert_graph(i, j))\n",
    "    args.max_prev_node = 20\n",
    "\n",
    "if args.graph_type == 'enzymes_small':\n",
    "    graphs_raw = Graph_load_batch(min_num_nodes=10, name='ENZYMES')\n",
    "    graphs = []\n",
    "    for G in graphs_raw:\n",
    "        if G.number_of_nodes()<=20:\n",
    "            graphs.append(G)\n",
    "    args.max_prev_node = 15\n",
    "\n",
    "if args.graph_type == 'citeseer_small':\n",
    "    _, _, G = Graph_load(dataset='citeseer')\n",
    "    G = G = max([G.subgraph(c).copy() for c in nx.connected_components(G)], key=len)\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    graphs = []\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        G_ego = nx.ego_graph(G, i, radius=1)\n",
    "        if (G_ego.number_of_nodes() >= 4) and (G_ego.number_of_nodes() <= 20):\n",
    "            graphs.append(G_ego)\n",
    "    shuffle(graphs)\n",
    "    graphs = graphs[0:200]\n",
    "    args.max_prev_node = 15\n",
    "\n",
    "\n",
    "# remove self loops\n",
    "for graph in graphs:\n",
    "    #edges_with_selfloops = graph.selfloop_edges() #1.1\n",
    "    edges_with_selfloops = list(nx.selfloop_edges(graph)) #3.3\n",
    "    if len(edges_with_selfloops) > 0:\n",
    "        graph.remove_edges_from(edges_with_selfloops)\n",
    "\n",
    "# split datasets\n",
    "random.seed(123)\n",
    "shuffle(graphs)\n",
    "graphs_len = len(graphs)\n",
    "graphs_test = graphs[int(0.8 * graphs_len):]\n",
    "graphs_train = graphs[0:int(0.8 * graphs_len)]\n",
    "\n",
    "args.max_num_node = max([graphs[i].number_of_nodes() for i in range(len(graphs))])\n",
    "# args.max_num_node = 2000\n",
    "# show graphs statistics\n",
    "print('total graph num: {}, training set: {}'.format(len(graphs), len(graphs_train)))\n",
    "print('max number node: {}'.format(args.max_num_node))\n",
    "print('max previous node: {}'.format(args.max_prev_node))\n",
    "\n",
    "\n",
    "### train\n",
    "train_DGMG(args,graphs,model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:33:59.970032200Z",
     "start_time": "2024-06-21T08:33:53.574022700Z"
    }
   },
   "id": "609378cc9f5945dc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {1: {}}, 1: {0: {}, 2: {}}, 2: {1: {}, 3: {}}, 3: {2: {}}}\n",
      "[[1], [0, 2], [1, 3], [2]]\n"
     ]
    }
   ],
   "source": [
    "# Warnings and changes: \n",
    "# depreceated: G.selfloop_edges()\n",
    "# now: list(nx.selfloop_edges(graph))\n",
    "\n",
    "# adjacency_list deprecated: \n",
    "# https://networkx.org/documentation/networkx-1.11/reference/generated/networkx.Graph.adjacency_list.html\n",
    "\n",
    "G = nx.path_graph(4) \n",
    "#nx.draw(G, with_labels=True)\n",
    "\n",
    "# Access the adjacency list\n",
    "adjacency_list = [[n for n in G.neighbors(node)] for node in G.nodes()]\n",
    "print(G.adj)\n",
    "\n",
    "print(adjacency_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:10:24.845085Z",
     "start_time": "2024-06-21T08:10:24.813736100Z"
    }
   },
   "id": "aef3c9a7cbb9359c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "graphs = []\n",
    "node_count = 10\n",
    "\n",
    "for i in range(2, 5):\n",
    "    for j in range(2, 6):\n",
    "        graphs.append(nx.grid_2d_graph(i, j))\n",
    "\n",
    "graph = graphs[3]\n",
    "# do random ordering: relabel nodes\n",
    "node_order = list(range(graph.number_of_nodes()))\n",
    "shuffle(node_order)\n",
    "order_mapping = dict(zip(graph.nodes(), node_order))\n",
    "graph = nx.relabel_nodes(graph, order_mapping, copy=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:12:38.900852500Z",
     "start_time": "2024-06-21T08:12:38.869604500Z"
    }
   },
   "id": "efa5d4bebe85e010"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "m1 = [[n for n in graph.subgraph(list(range(node_count))).neighbors(node)]\n",
    "                             for node in graph.subgraph(list(range(node_count))).nodes()]\n",
    "\n",
    "m1n = [[n for n in graph.subgraph(list(range(node_count+1))).neighbors(node)]\n",
    "                     for node in graph.subgraph(list(range(node_count+1))).nodes()][-1]\n",
    "\n",
    "m2 = graph.subgraph(list(range(node_count))).adjacency_list() \n",
    "m2n = graph.subgraph(list(range(node_count+1))).adjacency_list()[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:12:40.605534600Z",
     "start_time": "2024-06-21T08:12:40.574282900Z"
    }
   },
   "id": "2d1ac70fcb11c592"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4], [0, 2, 5], [1, 8], [0, 6, 9], [0, 5, 6], [1, 4, 8], [3, 4, 7], [6, 9], [2, 5], [7, 3]]\n",
      "[7, 3]\n",
      "::::::::::::::::::::::::::::::::::::::::\n",
      "[[1, 3, 4], [0, 2, 5], [1, 8], [0, 6, 9], [0, 5, 6], [1, 4, 8], [3, 4, 7], [6, 9], [2, 5], [7, 3]]\n",
      "[7, 3]\n"
     ]
    }
   ],
   "source": [
    "print(m1)\n",
    "print(m1n)\n",
    "print(\":\"*40)\n",
    "print(m2)\n",
    "print(m2n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T08:12:42.596335100Z",
     "start_time": "2024-06-21T08:12:42.565080400Z"
    }
   },
   "id": "bc54f2e25a8809e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
